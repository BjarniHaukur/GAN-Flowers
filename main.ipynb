{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d94db90",
   "metadata": {},
   "source": [
    "# REI505M Machine Learning - Final project\n",
    "### Due: --------\n",
    "\n",
    "**Names**: Axel Kristj√°n Axelsson, Bjarni Haukur Bjarnason <br />\n",
    "**Email**: aka30@hi.is, bhb23@hi.is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ad1d8",
   "metadata": {},
   "source": [
    "**1. Image classification**\n",
    "\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9288b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "tf.executing_eagerly()\n",
    "\n",
    "\n",
    "from data_loader import MyDataLoader\n",
    "from neural_nets import NeuralNets\n",
    "from helper_funcs import *\n",
    "from my_dataset import parse_to_image, parse_and_augment\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "norm = (128,128)\n",
    "\n",
    "dl = MyDataLoader(\"combine3\", norm)\n",
    "nn = NeuralNets(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl.normalize_train_data()\n",
    "# dl.all_to_one()\n",
    "# filenames = dl.get_all_filenames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from my_dataset import create_dataset_from_files\n",
    "\n",
    "# filenames = dl.get_all_filenames()\n",
    "\n",
    "# NUM_FILES = len(filenames)\n",
    "# VAL_SIZE = 762\n",
    "# TEST_SIZE = 320\n",
    "\n",
    "# index_arr1 = np.random.choice(NUM_FILES, VAL_SIZE)\n",
    "# index_arr2 = np.random.choice(NUM_FILES-VAL_SIZE, TEST_SIZE)\n",
    "\n",
    "# val_files = filenames[index_arr1]\n",
    "# train_files = np.delete(filenames, index_arr1)\n",
    "# test_files = train_files[index_arr2]\n",
    "# train_files = np.delete(train_files, index_arr2)\n",
    "# # NUM_FILES = len(filenames)\n",
    "\n",
    "# numpy_dump(train_files, \"med_train\")\n",
    "# numpy_dump(test_files, \"med_test\")\n",
    "# numpy_dump(val_files, \"med_val\")\n",
    "\n",
    "train_files = numpy_load(\"train_files\")\n",
    "test_files  = numpy_load(\"test_files\")\n",
    "val_files   = numpy_load(\"val_files\")\n",
    "\n",
    "# train_files = numpy_load(\"med_train\")\n",
    "# test_files = numpy_load(\"med_test\")\n",
    "# val_files = numpy_load(\"med_val\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "WORKERS    = 16\n",
    "TEST_SIZE  = test_files.shape[0]\n",
    "VAL_SIZE   = val_files.shape[0]\n",
    "TRAIN_SIZE = train_files.shape[0]\n",
    "NUM_FILES  = TEST_SIZE + VAL_SIZE + TRAIN_SIZE\n",
    "\n",
    "\n",
    "NUM_TRAIN_STEPS = math.floor(TRAIN_SIZE/BATCH_SIZE)\n",
    "NUM_VAL_STEPS   = math.floor(VAL_SIZE/BATCH_SIZE)\n",
    "\n",
    "ds_train = create_dataset_from_files(train_files, batch_size=BATCH_SIZE, num_parallel_calls=WORKERS, augment=True)\n",
    "ds_val   = create_dataset_from_files(val_files, batch_size=BATCH_SIZE, num_parallel_calls=WORKERS, augment=True)\n",
    "ds_test  = create_dataset_from_files(test_files, batch_size=BATCH_SIZE, num_parallel_calls=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSIMLoss(y_true, y_pred):\n",
    "  return (1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0)))#+0.5*tf.keras.metrics.mean_squared_error(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.model_i()\n",
    "# model.compile(optimizer='rmsprop', loss=SSIMLoss, metrics=[\"accuracy\"])\n",
    "# model.summary()\n",
    "model = load_model(\"model_i_SSIMV219\", custom={'SSIMLoss':SSIMLoss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "num_iters = 20\n",
    "\n",
    "test_images = ds_test.as_numpy_iterator().next()\n",
    "\n",
    "\n",
    "history_arr = np.ndarray(shape=(num_iters,), dtype=object)\n",
    "for i in range(num_iters):\n",
    "    history_arr[i] = model.fit(ds_train, epochs=5, steps_per_epoch=NUM_TRAIN_STEPS, validation_data=ds_val, validation_steps=NUM_VAL_STEPS)\n",
    "    \n",
    "    if i%2==0:\n",
    "        save_model(model, f\"model_i_SSIMV2_{i+20}\", brave=True)\n",
    "\n",
    "    img_number = randint(0, BATCH_SIZE-1)\n",
    "    img = np.expand_dims(test_images[0][img_number], 0)\n",
    "    pred = model.predict(img)\n",
    "    save_images(img , map_from(pred), name=\"model_i_SSIMV2\", enumerate=i+20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from helper_funcs import *\n",
    "\n",
    "# new_arr = np.ndarray(shape=(30,), dtype=object)\n",
    "# new_arr[:10] = history_arr\n",
    "# new_arr[10:] = history_arr2\n",
    "\n",
    "plot_acc_and_loss(history_arr, title=\"Model_i with 100% SSIM loss\", name=\"model_i_SSIMV2.2\", save=True, figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"model_i_SSIM_10\", custom={'SSIMLoss':SSIMLoss})\n",
    "# model = gan.get_generator()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725371bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_i_SSIMV219\", custom={'SSIMLoss':SSIMLoss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f874ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# model_e14 frekar gott general skill, model_e40 gott en of miklir litir\n",
    "####################################################################################\n",
    "####################################################################################\n",
    "# model_i_SSIM19 er skringilega gott\n",
    "####################################################################################\n",
    "\n",
    "# save_model(model, \"model_e40\")\n",
    "\n",
    "\n",
    "# img_numbers = np.random.choice(size, 5)\n",
    "# test_image = X_test[img_numbers]\n",
    "\n",
    "test_image = ds_test.as_numpy_iterator().next()\n",
    "\n",
    "\n",
    "\n",
    "num_images = 10\n",
    "\n",
    "# grey = test_image[0][[14, 16, 18, 30,31,32,32,34,35,36,37,38,39]]\n",
    "# ab   = test_image[1][[14, 16, 18, 30,31,32,32,34,35,36,37,38,39]]\n",
    "\n",
    "grey = test_image[0][:num_images]\n",
    "ab   = test_image[1][:num_images]\n",
    "none_arr = np.zeros(shape=(num_images, norm[0], norm[1], 2))\n",
    "\n",
    "pred = model.predict(grey)\n",
    "save_images(grey, none_arr, name=\"bw\")\n",
    "save_images(grey, map_from(pred), name=\"fake\", enhance=1.4)\n",
    "save_images(grey, map_from(ab), name=\"real\")\n",
    "# save_images(grey, map_from(ab), name=\"rgb\")\n",
    "# show_images(grey, map_from(ab))\n",
    "\n",
    "# show_images(grey, map_from(ab))\n",
    "# show_images(grey, map_from(pred), enhance=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e751cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "iter = ds_test.as_numpy_iterator()\n",
    "test_image = iter.next()\n",
    "\n",
    "num_images = 10\n",
    "\n",
    "grey = test_image[0][:num_images]\n",
    "ab   = test_image[1][:num_images]\n",
    "pred = model.predict(grey)\n",
    "for i in range(num_images):\n",
    "    rand = randint(0, 1)\n",
    "    if rand:\n",
    "        show_images(grey[i:i+1], map_from(ab[i:i+1]))\n",
    "    else:\n",
    "        show_images(grey[i:i+1], map_from(pred[i:i+1]))\n",
    "    print(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de146611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan import MyGAN\n",
    "gan = MyGAN(ds_train, ds_val, load_model(\"model_i_SSIMV219\", custom={'SSIMLoss':SSIMLoss}), nn.discriminator_b())\n",
    "# try:\n",
    "#     trained = gan.is_trained()\n",
    "#     if not trained:\n",
    "#         gan = MyGAN(ds_train, ds_val, nn.generator_b(), nn.discriminator_b())\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775529b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "num_iters = 20\n",
    "\n",
    "test_images = ds_test.as_numpy_iterator().next()\n",
    "\n",
    "gan.train_individually(0, 12, NUM_TRAIN_STEPS, NUM_VAL_STEPS)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    gan.fit(num_epochs=5, num_train_steps=NUM_TRAIN_STEPS, num_val_steps=NUM_VAL_STEPS)\n",
    "    \n",
    "    model = gan.get_generator()\n",
    "\n",
    "    if i%5==0:\n",
    "        save_model(model, f\"ganv2{i}\", brave=True)\n",
    "\n",
    "    img_number = randint(0, BATCH_SIZE-1)\n",
    "    img = np.expand_dims(test_images[0][img_number], 0)\n",
    "    pred = model(img)\n",
    "    save_images(img , map_from(pred), name=\"ganv2\", enumerate=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gan.get_generator()\n",
    "save_model(model, \"gan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorize import Colorize\n",
    "\n",
    "col = Colorize(\"sq.jpg\")\n",
    "\n",
    "col.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "# Create an SR object\n",
    "sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "\n",
    "# Read image\n",
    "image = cv2.imread('Ugla.jpg')\n",
    "\n",
    "# Read the desired model\n",
    "path = \"EDSR_x3.pb\"\n",
    "sr.readModel(path)\n",
    "\n",
    "# Set the desired model and scale to get correct pre- and post-processing\n",
    "sr.setModel(\"edsr\", 3)\n",
    "\n",
    "# Upscale the image\n",
    "result = sr.upsample(image)\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"./upscaled.png\", result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65d87dbd84d6699b6249583833f5a2128eb89a67d50c6406a8848397791798e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tf2.6.0': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
